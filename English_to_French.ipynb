{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-05-14T16:57:12.751135Z","iopub.execute_input":"2023-05-14T16:57:12.751499Z","iopub.status.idle":"2023-05-14T16:57:25.393505Z","shell.execute_reply.started":"2023-05-14T16:57:12.751468Z","shell.execute_reply":"2023-05-14T16:57:25.392194Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.11.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.28.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.64.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2.1.1)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.7.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 16nkAYoYkGRQk_bFY8z4E1087z0dElQbo","metadata":{"execution":{"iopub.status.busy":"2023-05-14T16:57:25.395846Z","iopub.execute_input":"2023-05-14T16:57:25.396251Z","iopub.status.idle":"2023-05-14T16:57:30.867668Z","shell.execute_reply.started":"2023-05-14T16:57:25.396212Z","shell.execute_reply":"2023-05-14T16:57:30.866478Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=16nkAYoYkGRQk_bFY8z4E1087z0dElQbo\nTo: /kaggle/working/eng_-french.csv\n100%|███████████████████████████████████████| 12.5M/12.5M [00:00<00:00, 206MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\nfrom tensorflow.keras import optimizers","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:17.840051Z","iopub.execute_input":"2023-05-14T19:42:17.840799Z","iopub.status.idle":"2023-05-14T19:42:17.846798Z","shell.execute_reply.started":"2023-05-14T19:42:17.840761Z","shell.execute_reply":"2023-05-14T19:42:17.845373Z"},"trusted":true},"execution_count":240,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/working/eng_-french.csv\")\ndata","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:18.467620Z","iopub.execute_input":"2023-05-14T19:42:18.467999Z","iopub.status.idle":"2023-05-14T19:42:18.777997Z","shell.execute_reply.started":"2023-05-14T19:42:18.467968Z","shell.execute_reply":"2023-05-14T19:42:18.777020Z"},"trusted":true},"execution_count":241,"outputs":[{"execution_count":241,"output_type":"execute_result","data":{"text/plain":"                                  English words/sentences  \\\n0                                                     Hi.   \n1                                                    Run!   \n2                                                    Run!   \n3                                                    Who?   \n4                                                    Wow!   \n...                                                   ...   \n175616  Top-down economics never works, said Obama. \"T...   \n175617  A carbon footprint is the amount of carbon dio...   \n175618  Death is something that we're often discourage...   \n175619  Since there are usually multiple websites on a...   \n175620  If someone who doesn't know your background sa...   \n\n                                   French words/sentences  \n0                                                  Salut!  \n1                                                 Cours !  \n2                                                Courez !  \n3                                                   Qui ?  \n4                                              Ça alors !  \n...                                                   ...  \n175616  « L'économie en partant du haut vers le bas, ç...  \n175617  Une empreinte carbone est la somme de pollutio...  \n175618  La mort est une chose qu'on nous décourage sou...  \n175619  Puisqu'il y a de multiples sites web sur chaqu...  \n175620  Si quelqu'un qui ne connaît pas vos antécédent...  \n\n[175621 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>English words/sentences</th>\n      <th>French words/sentences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hi.</td>\n      <td>Salut!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Run!</td>\n      <td>Cours !</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Run!</td>\n      <td>Courez !</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Who?</td>\n      <td>Qui ?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Wow!</td>\n      <td>Ça alors !</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>175616</th>\n      <td>Top-down economics never works, said Obama. \"T...</td>\n      <td>« L'économie en partant du haut vers le bas, ç...</td>\n    </tr>\n    <tr>\n      <th>175617</th>\n      <td>A carbon footprint is the amount of carbon dio...</td>\n      <td>Une empreinte carbone est la somme de pollutio...</td>\n    </tr>\n    <tr>\n      <th>175618</th>\n      <td>Death is something that we're often discourage...</td>\n      <td>La mort est une chose qu'on nous décourage sou...</td>\n    </tr>\n    <tr>\n      <th>175619</th>\n      <td>Since there are usually multiple websites on a...</td>\n      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n    </tr>\n    <tr>\n      <th>175620</th>\n      <td>If someone who doesn't know your background sa...</td>\n      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n    </tr>\n  </tbody>\n</table>\n<p>175621 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data['English words/sentences count'] = data['English words/sentences'].str.split().str.len()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:19.058481Z","iopub.execute_input":"2023-05-14T19:42:19.059606Z","iopub.status.idle":"2023-05-14T19:42:19.419615Z","shell.execute_reply.started":"2023-05-14T19:42:19.059559Z","shell.execute_reply":"2023-05-14T19:42:19.418573Z"},"trusted":true},"execution_count":242,"outputs":[]},{"cell_type":"code","source":"data['French words/sentences count'] = data['French words/sentences'].str.split().str.len()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:19.742632Z","iopub.execute_input":"2023-05-14T19:42:19.743016Z","iopub.status.idle":"2023-05-14T19:42:20.590382Z","shell.execute_reply.started":"2023-05-14T19:42:19.742983Z","shell.execute_reply":"2023-05-14T19:42:20.589394Z"},"trusted":true},"execution_count":243,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:20.592377Z","iopub.execute_input":"2023-05-14T19:42:20.592680Z","iopub.status.idle":"2023-05-14T19:42:20.639253Z","shell.execute_reply.started":"2023-05-14T19:42:20.592654Z","shell.execute_reply":"2023-05-14T19:42:20.637760Z"},"trusted":true},"execution_count":244,"outputs":[{"execution_count":244,"output_type":"execute_result","data":{"text/plain":"                                  English words/sentences  \\\n0                                                     Hi.   \n1                                                    Run!   \n2                                                    Run!   \n3                                                    Who?   \n4                                                    Wow!   \n...                                                   ...   \n175616  Top-down economics never works, said Obama. \"T...   \n175617  A carbon footprint is the amount of carbon dio...   \n175618  Death is something that we're often discourage...   \n175619  Since there are usually multiple websites on a...   \n175620  If someone who doesn't know your background sa...   \n\n                                   French words/sentences  \\\n0                                                  Salut!   \n1                                                 Cours !   \n2                                                Courez !   \n3                                                   Qui ?   \n4                                              Ça alors !   \n...                                                   ...   \n175616  « L'économie en partant du haut vers le bas, ç...   \n175617  Une empreinte carbone est la somme de pollutio...   \n175618  La mort est une chose qu'on nous décourage sou...   \n175619  Puisqu'il y a de multiples sites web sur chaqu...   \n175620  Si quelqu'un qui ne connaît pas vos antécédent...   \n\n        English words/sentences count  French words/sentences count  \n0                                   1                             1  \n1                                   1                             2  \n2                                   1                             2  \n3                                   1                             2  \n4                                   1                             3  \n...                               ...                           ...  \n175616                             34                            47  \n175617                             34                            33  \n175618                             37                            47  \n175619                             43                            49  \n175620                             44                            55  \n\n[175621 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>English words/sentences</th>\n      <th>French words/sentences</th>\n      <th>English words/sentences count</th>\n      <th>French words/sentences count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hi.</td>\n      <td>Salut!</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Run!</td>\n      <td>Cours !</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Run!</td>\n      <td>Courez !</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Who?</td>\n      <td>Qui ?</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Wow!</td>\n      <td>Ça alors !</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>175616</th>\n      <td>Top-down economics never works, said Obama. \"T...</td>\n      <td>« L'économie en partant du haut vers le bas, ç...</td>\n      <td>34</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>175617</th>\n      <td>A carbon footprint is the amount of carbon dio...</td>\n      <td>Une empreinte carbone est la somme de pollutio...</td>\n      <td>34</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>175618</th>\n      <td>Death is something that we're often discourage...</td>\n      <td>La mort est une chose qu'on nous décourage sou...</td>\n      <td>37</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>175619</th>\n      <td>Since there are usually multiple websites on a...</td>\n      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n      <td>43</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>175620</th>\n      <td>If someone who doesn't know your background sa...</td>\n      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n      <td>44</td>\n      <td>55</td>\n    </tr>\n  </tbody>\n</table>\n<p>175621 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#removing all the sentences greater than 6 words  \ndata = data[(data['English words/sentences count']<6) & (data['French words/sentences count']<6)]","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:20.640979Z","iopub.execute_input":"2023-05-14T19:42:20.641589Z","iopub.status.idle":"2023-05-14T19:42:20.654431Z","shell.execute_reply.started":"2023-05-14T19:42:20.641554Z","shell.execute_reply":"2023-05-14T19:42:20.653547Z"},"trusted":true},"execution_count":245,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:20.656485Z","iopub.execute_input":"2023-05-14T19:42:20.656921Z","iopub.status.idle":"2023-05-14T19:42:20.669801Z","shell.execute_reply.started":"2023-05-14T19:42:20.656877Z","shell.execute_reply":"2023-05-14T19:42:20.668562Z"},"trusted":true},"execution_count":246,"outputs":[{"execution_count":246,"output_type":"execute_result","data":{"text/plain":"                              English words/sentences  \\\n0                                                 Hi.   \n1                                                Run!   \n2                                                Run!   \n3                                                Who?   \n4                                                Wow!   \n...                                               ...   \n153098    Tom's great-great-grandfather was a pirate.   \n153107    Unfortunately, the information is accurate.   \n154934   They're having a going-out-of-business sale.   \n154962   This theory is scientifically controversial.   \n156500  The administration makes important decisions.   \n\n                                   French words/sentences  \\\n0                                                  Salut!   \n1                                                 Cours !   \n2                                                Courez !   \n3                                                   Qui ?   \n4                                              Ça alors !   \n...                                                   ...   \n153098  L'arrière-arrière-grand-père de Tom était pirate.   \n153107         Malheureusement, l'information est exacte.   \n154934                           Ils ont une liquidation.   \n154962   Cette théorie est scientifiquement controversée.   \n156500  L'administration prend des décisions importantes.   \n\n        English words/sentences count  French words/sentences count  \n0                                   1                             1  \n1                                   1                             2  \n2                                   1                             2  \n3                                   1                             2  \n4                                   1                             3  \n...                               ...                           ...  \n153098                              5                             5  \n153107                              5                             4  \n154934                              5                             4  \n154962                              5                             5  \n156500                              5                             5  \n\n[54212 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>English words/sentences</th>\n      <th>French words/sentences</th>\n      <th>English words/sentences count</th>\n      <th>French words/sentences count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hi.</td>\n      <td>Salut!</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Run!</td>\n      <td>Cours !</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Run!</td>\n      <td>Courez !</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Who?</td>\n      <td>Qui ?</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Wow!</td>\n      <td>Ça alors !</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>153098</th>\n      <td>Tom's great-great-grandfather was a pirate.</td>\n      <td>L'arrière-arrière-grand-père de Tom était pirate.</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>153107</th>\n      <td>Unfortunately, the information is accurate.</td>\n      <td>Malheureusement, l'information est exacte.</td>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>154934</th>\n      <td>They're having a going-out-of-business sale.</td>\n      <td>Ils ont une liquidation.</td>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>154962</th>\n      <td>This theory is scientifically controversial.</td>\n      <td>Cette théorie est scientifiquement controversée.</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>156500</th>\n      <td>The administration makes important decisions.</td>\n      <td>L'administration prend des décisions importantes.</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>54212 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data['English words/sentences count'].max()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:20.690259Z","iopub.execute_input":"2023-05-14T19:42:20.690540Z","iopub.status.idle":"2023-05-14T19:42:20.697460Z","shell.execute_reply.started":"2023-05-14T19:42:20.690515Z","shell.execute_reply":"2023-05-14T19:42:20.696289Z"},"trusted":true},"execution_count":247,"outputs":[{"execution_count":247,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}]},{"cell_type":"code","source":"#hete the duplicates are zero \ndata.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:20.870560Z","iopub.execute_input":"2023-05-14T19:42:20.870936Z","iopub.status.idle":"2023-05-14T19:42:20.906746Z","shell.execute_reply.started":"2023-05-14T19:42:20.870907Z","shell.execute_reply":"2023-05-14T19:42:20.905783Z"},"trusted":true},"execution_count":248,"outputs":[{"execution_count":248,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"data = data.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:21.058367Z","iopub.execute_input":"2023-05-14T19:42:21.058741Z","iopub.status.idle":"2023-05-14T19:42:21.094722Z","shell.execute_reply.started":"2023-05-14T19:42:21.058711Z","shell.execute_reply":"2023-05-14T19:42:21.093754Z"},"trusted":true},"execution_count":249,"outputs":[]},{"cell_type":"markdown","source":"# **Diving the Data in to Train Test Validate** ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain,test = train_test_split(data,test_size = 0.2,random_state = 42 )","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:25.778433Z","iopub.execute_input":"2023-05-14T19:42:25.778806Z","iopub.status.idle":"2023-05-14T19:42:25.795636Z","shell.execute_reply.started":"2023-05-14T19:42:25.778776Z","shell.execute_reply":"2023-05-14T19:42:25.794442Z"},"trusted":true},"execution_count":250,"outputs":[]},{"cell_type":"code","source":"train,valid = train_test_split(train,test_size= 0.2)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:26.818465Z","iopub.execute_input":"2023-05-14T19:42:26.818850Z","iopub.status.idle":"2023-05-14T19:42:26.834052Z","shell.execute_reply.started":"2023-05-14T19:42:26.818819Z","shell.execute_reply":"2023-05-14T19:42:26.833019Z"},"trusted":true},"execution_count":251,"outputs":[]},{"cell_type":"markdown","source":"## Word Frequency","metadata":{}},{"cell_type":"code","source":"freq_x= train['English words/sentences'].str.split(expand=True).stack().value_counts().reset_index()\nfreq_y= train['French words/sentences'].str.split(expand=True).stack().value_counts().reset_index()\nfreq_x.to_csv('/kaggle/working/English_freq.csv',index=False) \nfreq_y.to_csv('/kaggle/working/French_freq.csv',index=False) ","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:30.868768Z","iopub.execute_input":"2023-05-14T19:42:30.869190Z","iopub.status.idle":"2023-05-14T19:42:31.183056Z","shell.execute_reply.started":"2023-05-14T19:42:30.869153Z","shell.execute_reply":"2023-05-14T19:42:31.181944Z"},"trusted":true},"execution_count":252,"outputs":[]},{"cell_type":"code","source":"freq_x = pd.read_csv('/kaggle/working/English_freq.csv')\nfreq_y = pd.read_csv('/kaggle/working/French_freq.csv')","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:31.185004Z","iopub.execute_input":"2023-05-14T19:42:31.185372Z","iopub.status.idle":"2023-05-14T19:42:31.209393Z","shell.execute_reply.started":"2023-05-14T19:42:31.185339Z","shell.execute_reply":"2023-05-14T19:42:31.208517Z"},"trusted":true},"execution_count":253,"outputs":[]},{"cell_type":"code","source":"freq_x","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:31.211001Z","iopub.execute_input":"2023-05-14T19:42:31.211374Z","iopub.status.idle":"2023-05-14T19:42:31.221869Z","shell.execute_reply.started":"2023-05-14T19:42:31.211342Z","shell.execute_reply":"2023-05-14T19:42:31.221003Z"},"trusted":true},"execution_count":254,"outputs":[{"execution_count":254,"output_type":"execute_result","data":{"text/plain":"           index     0\n0              I  5994\n1            you  3393\n2              a  3328\n3             is  2943\n4            the  2292\n...          ...   ...\n10421    Kissing     1\n10422   forceful     1\n10423      ruins     1\n10424       Diet     1\n10425  objected.     1\n\n[10426 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I</td>\n      <td>5994</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>you</td>\n      <td>3393</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a</td>\n      <td>3328</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>is</td>\n      <td>2943</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>the</td>\n      <td>2292</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10421</th>\n      <td>Kissing</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10422</th>\n      <td>forceful</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10423</th>\n      <td>ruins</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10424</th>\n      <td>Diet</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10425</th>\n      <td>objected.</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>10426 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"freq_y","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:31.328297Z","iopub.execute_input":"2023-05-14T19:42:31.328618Z","iopub.status.idle":"2023-05-14T19:42:31.341934Z","shell.execute_reply.started":"2023-05-14T19:42:31.328591Z","shell.execute_reply":"2023-05-14T19:42:31.340742Z"},"trusted":true},"execution_count":255,"outputs":[{"execution_count":255,"output_type":"execute_result","data":{"text/plain":"           index     0\n0              ?  5204\n1             Je  4625\n2            est  2320\n3             Il  2249\n4            pas  2197\n...          ...   ...\n16322    draguer     1\n16323  retrouver     1\n16324    rentres     1\n16325     Pensez     1\n16326  distrait,     1\n\n[16327 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>?</td>\n      <td>5204</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Je</td>\n      <td>4625</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>est</td>\n      <td>2320</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Il</td>\n      <td>2249</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>pas</td>\n      <td>2197</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16322</th>\n      <td>draguer</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16323</th>\n      <td>retrouver</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16324</th>\n      <td>rentres</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16325</th>\n      <td>Pensez</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16326</th>\n      <td>distrait,</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>16327 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## **Data Prepration**","metadata":{}},{"cell_type":"code","source":"def get_data(raw_lines) : \n    text = [] \n    for raw_line in raw_lines:\n        text.append('<start> '+ raw_line +' <end>')\n    return text ","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:32.959290Z","iopub.execute_input":"2023-05-14T19:42:32.959728Z","iopub.status.idle":"2023-05-14T19:42:32.965052Z","shell.execute_reply.started":"2023-05-14T19:42:32.959694Z","shell.execute_reply":"2023-05-14T19:42:32.964093Z"},"trusted":true},"execution_count":256,"outputs":[]},{"cell_type":"code","source":"#english_train\n#French_train\nenglish_train = get_data(list(train['English words/sentences']))\nfrench_train = get_data(list(train['French words/sentences']))","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:33.120622Z","iopub.execute_input":"2023-05-14T19:42:33.121309Z","iopub.status.idle":"2023-05-14T19:42:33.172916Z","shell.execute_reply.started":"2023-05-14T19:42:33.121277Z","shell.execute_reply":"2023-05-14T19:42:33.172039Z"},"trusted":true},"execution_count":257,"outputs":[]},{"cell_type":"code","source":"english_valid = get_data(list(valid['English words/sentences']))\nfrench_valid = get_data(list(valid['French words/sentences']))","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:33.250345Z","iopub.execute_input":"2023-05-14T19:42:33.250904Z","iopub.status.idle":"2023-05-14T19:42:33.267461Z","shell.execute_reply.started":"2023-05-14T19:42:33.250867Z","shell.execute_reply":"2023-05-14T19:42:33.266456Z"},"trusted":true},"execution_count":258,"outputs":[]},{"cell_type":"markdown","source":"# **Tokenization**","metadata":{}},{"cell_type":"code","source":"#tokenisation and padding Input language \nfre_token = Tokenizer(filters='',lower=False)\nfre_token.fit_on_texts(french_train)\nfre_tokenized = fre_token.texts_to_sequences(french_train)\nfre_padded = pad_sequences(fre_tokenized,padding='post')\n\n#tokenisation and padding language language \neng_token = Tokenizer(filters='',lower=False)\neng_token.fit_on_texts(english_train)\neng_tokenized = fre_token.texts_to_sequences(english_train)\neng_padded = pad_sequences(eng_tokenized,padding='post')","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:35.138824Z","iopub.execute_input":"2023-05-14T19:42:35.139213Z","iopub.status.idle":"2023-05-14T19:42:36.697997Z","shell.execute_reply.started":"2023-05-14T19:42:35.139182Z","shell.execute_reply":"2023-05-14T19:42:36.697040Z"},"trusted":true},"execution_count":259,"outputs":[]},{"cell_type":"code","source":"#number of input tokens in the input & output language\nnum_op_tokens = len(fre_token.word_index)\nnum_ip_tokens = len(eng_token.word_index)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:38.048705Z","iopub.execute_input":"2023-05-14T19:42:38.049112Z","iopub.status.idle":"2023-05-14T19:42:38.054655Z","shell.execute_reply.started":"2023-05-14T19:42:38.049078Z","shell.execute_reply":"2023-05-14T19:42:38.053705Z"},"trusted":true},"execution_count":260,"outputs":[]},{"cell_type":"code","source":"print(num_op_tokens)\nprint(num_ip_tokens)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:38.240452Z","iopub.execute_input":"2023-05-14T19:42:38.241076Z","iopub.status.idle":"2023-05-14T19:42:38.247347Z","shell.execute_reply.started":"2023-05-14T19:42:38.241014Z","shell.execute_reply":"2023-05-14T19:42:38.246228Z"},"trusted":true},"execution_count":261,"outputs":[{"name":"stdout","text":"17221\n10428\n","output_type":"stream"}]},{"cell_type":"code","source":"#maximum length of sentence in both sentences \nmax_len_op = fre_padded.shape[1]\nmax_len_ip = eng_padded.shape[1]\n","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:39.787082Z","iopub.execute_input":"2023-05-14T19:42:39.788148Z","iopub.status.idle":"2023-05-14T19:42:39.794121Z","shell.execute_reply.started":"2023-05-14T19:42:39.788096Z","shell.execute_reply":"2023-05-14T19:42:39.792260Z"},"trusted":true},"execution_count":262,"outputs":[]},{"cell_type":"code","source":"print(max_len_op)\nprint(max_len_ip)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:39.938395Z","iopub.execute_input":"2023-05-14T19:42:39.938713Z","iopub.status.idle":"2023-05-14T19:42:39.945202Z","shell.execute_reply.started":"2023-05-14T19:42:39.938685Z","shell.execute_reply":"2023-05-14T19:42:39.943997Z"},"trusted":true},"execution_count":263,"outputs":[{"name":"stdout","text":"7\n6\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **KText**","metadata":{}},{"cell_type":"code","source":"#from tensorflow.keras.preprocessing.sequence import pad_sequences\n#from ktext.preprocess import processor","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:41.799805Z","iopub.execute_input":"2023-05-14T19:42:41.800195Z","iopub.status.idle":"2023-05-14T19:42:41.805279Z","shell.execute_reply.started":"2023-05-14T19:42:41.800163Z","shell.execute_reply":"2023-05-14T19:42:41.804010Z"},"trusted":true},"execution_count":264,"outputs":[]},{"cell_type":"code","source":"#english_pp = processor(keep_n=10429, padding_maxlen=6)\n#english_train_vecs english_pp.fit_transform(english_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:41.970348Z","iopub.execute_input":"2023-05-14T19:42:41.970947Z","iopub.status.idle":"2023-05-14T19:42:41.976068Z","shell.execute_reply.started":"2023-05-14T19:42:41.970907Z","shell.execute_reply":"2023-05-14T19:42:41.974733Z"},"trusted":true},"execution_count":265,"outputs":[]},{"cell_type":"code","source":"# # Title\n# french_pp = processor (append_indicators=True, keep_n=17226, padding_maxlen=7, padding = 'post')\n# # process the title data french_train\n# import dill as_dpickle\n# import numpy as np\n# # Save the preprocessor\n# with open('english_pp.dpkl', 'wb') as f: \n#     dpickle.dump(english_pp, f)\n# with open('french_pp.dpk1', 'wb') as f: \n#     dpickle.dump(french_pp, f)\n# # Save the processed data\n# np.save('french_train_vecs.npy', french_train_vecs) \n#np.save('english_train_vecs.npy', english_train_vecs)_vecs french_pp.fit_transform(french_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:42.138391Z","iopub.execute_input":"2023-05-14T19:42:42.138979Z","iopub.status.idle":"2023-05-14T19:42:42.144504Z","shell.execute_reply.started":"2023-05-14T19:42:42.138939Z","shell.execute_reply":"2023-05-14T19:42:42.143539Z"},"trusted":true},"execution_count":266,"outputs":[]},{"cell_type":"code","source":"# def load_decoder_inputs(decoder_np_vecs= 'french_train_vecs.npy'):\n#     vectorized_title = np.load(decoder_np_vecs)\n#     # For Decoder Input, you don't need the last word as that is only for prediction # when we are training using Teacher Forcing. \n#     decoder_input_data = vectorized_title[:, -1]\n#     # Decoder Target Data Is Ahead By 1 Time Step From Decoder Input Data (Teacher Forcing) \n#     decoder_target_data = vectorized_title[:, 1:]\n#     print(f'Shape of decoder input: {decoder_input_data.shape}') \n#     print(f'Shape of decoder target: {decoder_target_data.shape}') \n#     return decoder_input_data, decoder_target_data","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:42.288461Z","iopub.execute_input":"2023-05-14T19:42:42.289047Z","iopub.status.idle":"2023-05-14T19:42:42.294469Z","shell.execute_reply.started":"2023-05-14T19:42:42.288991Z","shell.execute_reply":"2023-05-14T19:42:42.293507Z"},"trusted":true},"execution_count":267,"outputs":[]},{"cell_type":"code","source":"# def load_encoder_inputs(encoder_np_vecs= 'english_train_vecs.npy'):\n#     vectorized_body = np.load(encoder_np_vecs)\n#     # Encoder input is simply the body of the issue text encoder_input_data = vectorized_body\n#     doc_length = encoder_input_data.shape[1] \n#     print (f'Shape of encoder input: {encoder_input_data.shape}') \n# return encoder_input_data, doc_length\n   ","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:42.429166Z","iopub.execute_input":"2023-05-14T19:42:42.429452Z","iopub.status.idle":"2023-05-14T19:42:42.433969Z","shell.execute_reply.started":"2023-05-14T19:42:42.429427Z","shell.execute_reply":"2023-05-14T19:42:42.433070Z"},"trusted":true},"execution_count":268,"outputs":[]},{"cell_type":"code","source":"# def load_text_processor (fname='english_pp.dpk1'):\n# # Load files from disk \n# with open(fname, 'rb') as f:\n#     pp dpickle.load(f)\n# num_tokens = max(pp.id2token.keys()) + 1 \n# print (f'Size of vocabulary for {fname}: {num_tokens:,}') \n# return num_tokens, pp\n","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:43.818811Z","iopub.execute_input":"2023-05-14T19:42:43.819688Z","iopub.status.idle":"2023-05-14T19:42:43.824524Z","shell.execute_reply.started":"2023-05-14T19:42:43.819641Z","shell.execute_reply":"2023-05-14T19:42:43.823542Z"},"trusted":true},"execution_count":269,"outputs":[]},{"cell_type":"code","source":"# num_encoder_tokens, english_pp = load_text_processor('english_pp.dpk1') \n# num_decoder_tokens, french_pp = load_text_processor('french_pp.dpk1')","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:43.990300Z","iopub.execute_input":"2023-05-14T19:42:43.990909Z","iopub.status.idle":"2023-05-14T19:42:43.996523Z","shell.execute_reply.started":"2023-05-14T19:42:43.990867Z","shell.execute_reply":"2023-05-14T19:42:43.995108Z"},"trusted":true},"execution_count":270,"outputs":[]},{"cell_type":"markdown","source":"# Machine Translation Model","metadata":{}},{"cell_type":"code","source":"latent_dim = 256\n\n# Encoder model \nencoder_inputs = Input(shape=(max_len_ip,), name='Encoder-Input')\n\n# Word embedding for encoder \nx = Embedding(num_ip_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\nx = BatchNormalization(name='Encoder-Batchnorm-1')(x) \n\n# Intermediate GRU layer(optional)\n_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n\n# Encapsulate the encoder as a separate entity so we can just\n# encode without decoding if we want to \nencoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\nseq2seq_encoder_out = encoder_model(encoder_inputs) ","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:46.298844Z","iopub.execute_input":"2023-05-14T19:42:46.299246Z","iopub.status.idle":"2023-05-14T19:42:46.830811Z","shell.execute_reply.started":"2023-05-14T19:42:46.299214Z","shell.execute_reply":"2023-05-14T19:42:46.829803Z"},"trusted":true},"execution_count":271,"outputs":[]},{"cell_type":"code","source":"# Decoder Model \ndecoder_inputs = Input(shape=(None,), name='Decoder-Input')  # For teacher forcing \n\n# Word Embedding for the Decoder \ndec_emb = Embedding(num_op_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\ndec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n\n# Set up the decoder, using `decoder_state_inputs` as initial state\ndecoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\ndecoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\nx = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n\n# Dense layer for prediction \ndecoder_dense = Dense(num_op_tokens, activation='softmax',name=\"Final-Output-Dense\")\ndecoder_outputs = decoder_dense (x)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:48.340455Z","iopub.execute_input":"2023-05-14T19:42:48.340893Z","iopub.status.idle":"2023-05-14T19:42:48.724356Z","shell.execute_reply.started":"2023-05-14T19:42:48.340857Z","shell.execute_reply":"2023-05-14T19:42:48.723421Z"},"trusted":true},"execution_count":272,"outputs":[]},{"cell_type":"code","source":"#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out]) \nseq2seq_Model = Model ([encoder_inputs, decoder_inputs], decoder_outputs)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:52.698503Z","iopub.execute_input":"2023-05-14T19:42:52.698891Z","iopub.status.idle":"2023-05-14T19:42:52.708522Z","shell.execute_reply.started":"2023-05-14T19:42:52.698860Z","shell.execute_reply":"2023-05-14T19:42:52.707525Z"},"trusted":true},"execution_count":273,"outputs":[]},{"cell_type":"code","source":"seq2seq_Model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:54.088232Z","iopub.execute_input":"2023-05-14T19:42:54.088610Z","iopub.status.idle":"2023-05-14T19:42:54.118510Z","shell.execute_reply.started":"2023-05-14T19:42:54.088579Z","shell.execute_reply":"2023-05-14T19:42:54.117756Z"},"trusted":true},"execution_count":274,"outputs":[{"name":"stdout","text":"Model: \"model_27\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n Decoder-Input (InputLayer)     [(None, None)]       0           []                               \n                                                                                                  \n Decoder-Word-Embedding (Embedd  (None, None, 256)   4408576     ['Decoder-Input[0][0]']          \n ing)                                                                                             \n                                                                                                  \n Encoder-Input (InputLayer)     [(None, 6)]          0           []                               \n                                                                                                  \n Decoder-Batchnorm-1 (BatchNorm  (None, None, 256)   1024        ['Decoder-Word-Embedding[0][0]'] \n alization)                                                                                       \n                                                                                                  \n Encoder-Model (Functional)     (None, 256)          3065344     ['Encoder-Input[0][0]']          \n                                                                                                  \n Decoder-GRU (GRU)              [(None, None, 256),  394752      ['Decoder-Batchnorm-1[0][0]',    \n                                 (None, 256)]                     'Encoder-Model[0][0]']          \n                                                                                                  \n Decoder-Batchnorm-2 (BatchNorm  (None, None, 256)   1024        ['Decoder-GRU[0][0]']            \n alization)                                                                                       \n                                                                                                  \n Final-Output-Dense (Dense)     (None, None, 17221)  4425797     ['Decoder-Batchnorm-2[0][0]']    \n                                                                                                  \n==================================================================================================\nTotal params: 12,296,517\nTrainable params: 12,294,981\nNon-trainable params: 1,536\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Choose the data for training\nencoder_input_train = eng_padded\ndecoder_input_train = fre_padded[:, :-1]\ndecoder_target_train = fre_padded[:, 1:]\n\n# Train the model\nbatch_size = 64\nepochs = 30\nseq2seq_Model.compile(optimizer=optimizers.Adam(lr=0.001), loss='sparse_categorical_crossentropy')\nseq2seq_Model.fit([encoder_input_train, decoder_input_train], decoder_target_train,\n                  batch_size=batch_size, epochs=epochs, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:42:56.629921Z","iopub.execute_input":"2023-05-14T19:42:56.630909Z","iopub.status.idle":"2023-05-14T19:47:23.029104Z","shell.execute_reply.started":"2023-05-14T19:42:56.630872Z","shell.execute_reply":"2023-05-14T19:47:23.028097Z"},"trusted":true},"execution_count":275,"outputs":[{"name":"stdout","text":"Epoch 1/30\nWARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x78711d556b90> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: closure mismatch, requested ('self', 'step_function'), but source function had ()\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n434/434 [==============================] - ETA: 0s - loss: 4.2160WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x78711d5572e0> and will run it as-is.\nCause: Unable to locate the source code of <function Model.make_test_function.<locals>.test_function at 0x78711d5572e0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: lineno is out of bounds\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n434/434 [==============================] - 21s 34ms/step - loss: 4.2160 - val_loss: nan\nEpoch 2/30\n434/434 [==============================] - 7s 17ms/step - loss: 3.0300 - val_loss: nan\nEpoch 3/30\n434/434 [==============================] - 7s 16ms/step - loss: 2.4995 - val_loss: nan\nEpoch 4/30\n434/434 [==============================] - 7s 16ms/step - loss: 2.1875 - val_loss: nan\nEpoch 5/30\n434/434 [==============================] - 6s 15ms/step - loss: 2.0183 - val_loss: nan\nEpoch 6/30\n434/434 [==============================] - 7s 16ms/step - loss: 1.9253 - val_loss: nan\nEpoch 7/30\n434/434 [==============================] - 6s 15ms/step - loss: 1.8683 - val_loss: nan\nEpoch 8/30\n434/434 [==============================] - 7s 15ms/step - loss: 1.8280 - val_loss: nan\nEpoch 9/30\n434/434 [==============================] - 7s 15ms/step - loss: 1.7977 - val_loss: nan\nEpoch 10/30\n434/434 [==============================] - 7s 16ms/step - loss: 1.7760 - val_loss: nan\nEpoch 11/30\n434/434 [==============================] - 7s 16ms/step - loss: 1.7579 - val_loss: nan\nEpoch 12/30\n434/434 [==============================] - 7s 15ms/step - loss: 1.7433 - val_loss: nan\nEpoch 13/30\n434/434 [==============================] - 6s 15ms/step - loss: 1.7283 - val_loss: nan\nEpoch 14/30\n434/434 [==============================] - 6s 15ms/step - loss: 1.7173 - val_loss: nan\nEpoch 15/30\n434/434 [==============================] - 7s 16ms/step - loss: 1.7066 - val_loss: nan\nEpoch 16/30\n434/434 [==============================] - 7s 16ms/step - loss: 1.6990 - val_loss: nan\nEpoch 17/30\n434/434 [==============================] - 6s 14ms/step - loss: 1.6877 - val_loss: nan\nEpoch 18/30\n434/434 [==============================] - 7s 16ms/step - loss: 1.6803 - val_loss: nan\nEpoch 19/30\n434/434 [==============================] - 6s 15ms/step - loss: 1.6728 - val_loss: nan\nEpoch 20/30\n434/434 [==============================] - 6s 15ms/step - loss: 1.6652 - val_loss: nan\nEpoch 21/30\n434/434 [==============================] - 7s 16ms/step - loss: 1.6585 - val_loss: nan\nEpoch 22/30\n434/434 [==============================] - 6s 14ms/step - loss: 1.6510 - val_loss: nan\nEpoch 23/30\n434/434 [==============================] - 6s 15ms/step - loss: 1.6462 - val_loss: nan\nEpoch 24/30\n434/434 [==============================] - 6s 14ms/step - loss: 1.6406 - val_loss: nan\nEpoch 25/30\n434/434 [==============================] - 6s 15ms/step - loss: 1.6338 - val_loss: nan\nEpoch 26/30\n434/434 [==============================] - 7s 16ms/step - loss: 1.6294 - val_loss: nan\nEpoch 27/30\n434/434 [==============================] - 7s 15ms/step - loss: 1.6241 - val_loss: nan\nEpoch 28/30\n434/434 [==============================] - 6s 14ms/step - loss: 1.6204 - val_loss: nan\nEpoch 29/30\n434/434 [==============================] - 6s 14ms/step - loss: 1.6143 - val_loss: nan\nEpoch 30/30\n434/434 [==============================] - 7s 15ms/step - loss: 1.6098 - val_loss: nan\n","output_type":"stream"},{"execution_count":275,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x786fdf7e4af0>"},"metadata":{}}]},{"cell_type":"code","source":"del seq2seq_Model","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:47:23.031387Z","iopub.execute_input":"2023-05-14T19:47:23.031772Z","iopub.status.idle":"2023-05-14T19:47:23.036920Z","shell.execute_reply.started":"2023-05-14T19:47:23.031737Z","shell.execute_reply":"2023-05-14T19:47:23.035969Z"},"trusted":true},"execution_count":276,"outputs":[]},{"cell_type":"code","source":"#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\nseq2seq_Model = Model ([encoder_inputs, decoder_inputs], decoder_outputs) \nseq2seq_Model.compile(optimizer=optimizers.Adam(lr=0.001), loss='sparse_categorical_crossentropy')","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:49:50.105319Z","iopub.execute_input":"2023-05-14T19:49:50.105800Z","iopub.status.idle":"2023-05-14T19:49:50.124712Z","shell.execute_reply.started":"2023-05-14T19:49:50.105764Z","shell.execute_reply":"2023-05-14T19:49:50.123558Z"},"trusted":true},"execution_count":278,"outputs":[]},{"cell_type":"code","source":"seq2seq_Model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:49:51.706098Z","iopub.execute_input":"2023-05-14T19:49:51.706480Z","iopub.status.idle":"2023-05-14T19:49:51.737707Z","shell.execute_reply.started":"2023-05-14T19:49:51.706442Z","shell.execute_reply":"2023-05-14T19:49:51.736902Z"},"trusted":true},"execution_count":279,"outputs":[{"name":"stdout","text":"Model: \"model_28\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n Decoder-Input (InputLayer)     [(None, None)]       0           []                               \n                                                                                                  \n Decoder-Word-Embedding (Embedd  (None, None, 256)   4408576     ['Decoder-Input[0][0]']          \n ing)                                                                                             \n                                                                                                  \n Encoder-Input (InputLayer)     [(None, 6)]          0           []                               \n                                                                                                  \n Decoder-Batchnorm-1 (BatchNorm  (None, None, 256)   1024        ['Decoder-Word-Embedding[0][0]'] \n alization)                                                                                       \n                                                                                                  \n Encoder-Model (Functional)     (None, 256)          3065344     ['Encoder-Input[0][0]']          \n                                                                                                  \n Decoder-GRU (GRU)              [(None, None, 256),  394752      ['Decoder-Batchnorm-1[0][0]',    \n                                 (None, 256)]                     'Encoder-Model[0][0]']          \n                                                                                                  \n Decoder-Batchnorm-2 (BatchNorm  (None, None, 256)   1024        ['Decoder-GRU[0][0]']            \n alization)                                                                                       \n                                                                                                  \n Final-Output-Dense (Dense)     (None, None, 17221)  4425797     ['Decoder-Batchnorm-2[0][0]']    \n                                                                                                  \n==================================================================================================\nTotal params: 12,296,517\nTrainable params: 12,294,981\nNon-trainable params: 1,536\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"#seq2seq_Model.load_weights ('tutorial_seq2seq.epoch23-val1.78877.hdf5')","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:50:24.496845Z","iopub.execute_input":"2023-05-14T19:50:24.497244Z","iopub.status.idle":"2023-05-14T19:50:24.503902Z","shell.execute_reply.started":"2023-05-14T19:50:24.497212Z","shell.execute_reply":"2023-05-14T19:50:24.503007Z"},"trusted":true},"execution_count":281,"outputs":[]},{"cell_type":"code","source":"def extract_encoder_model(model):\n    encoder_model = model.get_layer('Encoder-Model')\n    return encoder_model\n\ndef extract_decoder_model(model):\n    # the Latent dimension is the same throughout the architecture so we are going to\n    # cheat and grab the latent dimension of the embedding because that is the same as who\n    #output from the decoder\n    latent_dim = model.get_layer('Decoder-Word-Embedding').output_shape[-1]\n    # Reconstruct the input into the decoder \n    decoder_inputs = model.get_layer ('Decoder-Input').input\n    dec_emb = model.get_layer('Decoder-Word-Embedding') (decoder_inputs) \n    dec_bn = model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n   \n    gru_inference_state_input = Input(shape=latent_dim,name='hidden_state_input')\n    gru_out, gru_state_out = model.get_layer('Decoder-GRU') ([dec_bn, gru_inference_state_input])\n    \n    # Reconstruct dense Layers\n    dec_bn2 = model.get_layer ('Decoder-Batchnorm-2')(gru_out) \n    dense_out = model.get_layer('Final-Output-Dense')(dec_bn2) \n    decoder_model = Model ([decoder_inputs, gru_inference_state_input], [dense_out, gru_state_out])\n    return decoder_model","metadata":{"execution":{"iopub.status.busy":"2023-05-14T19:52:32.599956Z","iopub.execute_input":"2023-05-14T19:52:32.600575Z","iopub.status.idle":"2023-05-14T19:52:32.616340Z","shell.execute_reply.started":"2023-05-14T19:52:32.600528Z","shell.execute_reply":"2023-05-14T19:52:32.615184Z"},"trusted":true},"execution_count":292,"outputs":[]},{"cell_type":"markdown","source":"### **Sequence2Sequence Inference**","metadata":{}},{"cell_type":"code","source":"class Seq2Seq_Inference(object):\n    def __init__(self, encoder_preprocessor, decoder_preprocessor, seq2seq_model):\n        self.pp_english = encoder_preprocessor\n        self.pp_french = decoder_preprocessor\n        self.seq2seq_model = seq2seq_model\n        self.encoder_model = extract_encoder_model(seq2seq_model)\n        self.decoder_model = extract_decoder_model(seq2seq_model)\n        self.default_max_len_french = len(self.pp_french.word_index)+1\n        self.nn = None\n        self.rec_df = None\n\n    def generate_french(self, raw_input_text, max_len_french=None):\n        if max_len_french is None:\n            max_len_french = self.default_max_len_french\n        raw_tokenized = self.pp_english.texts_to_sequences(raw_input_text)\n        temp = pad_sequences(raw_tokenized,padding='post',maxlen=6)\n        body_encoding = self.encoder_model.predict(temp)\n    \n        body_encoding = body_encoding[0]\n        \n        print(body_encoding.shape)\n        state_value = np.array(self.pp_french.texts_to_sequences(['<start>'])).reshape(1, 1)\n        print(state_value.shape)\n        decoded_sentence = []\n        stop_condition = False\n\n        while not stop_condition:\n            preds, st = self.decoder_model.predict([state_value, body_encoding])\n            pred_idx = np.argmax(preds[:, :, 2:]) + 2\n            pred_word_str = self.pp_french.sequences_to_texts[pred_idx]\n            if pred_word_str == '<end>' or len(decoded_sentence) >= max_len_french:\n                stop_condition = True\n                break\n\n            decoded_sentence.append(pred_word_str)\n            # Update the decoder for the next word\n            body_encoding = st\n            state_value = np.array(pred_idx).reshape(1, 1)\n\n        return original_body_encoding, ' '.join(decoded_sentence)\n\n    def print_example(self, i, english_text, french_text=None, threshold=None):\n        if i:\n            print('\\n\\n====================================')\n            print(f'==============Example # {i}==============')\n\n        print(f\"English Text:\\n{english_text} \\n\")\n\n        if french_text:\n            print(f\"Original French Text:\\n{french_text}\")\n\n        emb, gen_french = self.generate_french(english_text)\n        print(f\"\\n**********Generated French Text***********:\\n{gen_french}\")\n        \n        \n        if self.nn:\n            # return neighbors and distances\n            n, d = self.nn.get_nns_by_vector(emb.flatten(), n=4, include_distances=True)\n            neighbors =n[1:]\n            dist = d[1:]\n            if min(dist) <= threshold:\n                cols = ['French', 'English']\n                dfcopy = self.rec_df.iloc[neighbors ] [cols].copy(deep=True)\n                dfcopy ['dist'] = dist\n                similar_issues_df = dfcopy.query(f'dist <= {threshold}')\n                display(similar_issues_df)\n                \n    def demo_model_predictions(self,n,df,threshold=None):\n        # Extract body and title from DF\n        english_text_list = df['English words/sentences'].tolist() \n        french_text_list = df['French words/sentences'].tolist()\n        demo_list = np.random.randint(low=1, high=len(english_text_list), size=n)\n        for i in demo_list:\n            self.print_example(i,english_text=english_text_list[i], french_text=french_text_list[i], threshold=threshold)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:31:27.534208Z","iopub.execute_input":"2023-05-14T20:31:27.534953Z","iopub.status.idle":"2023-05-14T20:31:27.554247Z","shell.execute_reply.started":"2023-05-14T20:31:27.534890Z","shell.execute_reply":"2023-05-14T20:31:27.552624Z"},"trusted":true},"execution_count":381,"outputs":[]},{"cell_type":"code","source":"seq2seq_inf = Seq2Seq_Inference (encoder_preprocessor=eng_token, decoder_preprocessor=fre_token, seq2seq_model=seq2seq_Model)\n# this method displays the predictions on random rows of the holdout set","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:31:27.678262Z","iopub.execute_input":"2023-05-14T20:31:27.678566Z","iopub.status.idle":"2023-05-14T20:31:27.924380Z","shell.execute_reply.started":"2023-05-14T20:31:27.678540Z","shell.execute_reply":"2023-05-14T20:31:27.923413Z"},"trusted":true},"execution_count":382,"outputs":[]},{"cell_type":"code","source":"# this method displays the predictions on random rows of the holdout set\nseq2seq_inf.demo_model_predictions(n=50, df=test, threshold=0.3)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-14T20:31:27.926270Z","iopub.execute_input":"2023-05-14T20:31:27.926783Z","iopub.status.idle":"2023-05-14T20:31:28.065735Z","shell.execute_reply.started":"2023-05-14T20:31:27.926746Z","shell.execute_reply":"2023-05-14T20:31:28.063309Z"},"trusted":true},"execution_count":383,"outputs":[{"name":"stdout","text":"\n\n====================================\n==============Example # 4870==============\nEnglish Text:\nI know what they are. \n\nOriginal French Text:\nJe sais ce qu'elles sont.\n1/1 [==============================] - 0s 20ms/step\n(256,)\n(1, 1)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[383], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# this method displays the predictions on random rows of the holdout set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mseq2seq_inf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdemo_model_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[381], line 74\u001b[0m, in \u001b[0;36mSeq2Seq_Inference.demo_model_predictions\u001b[0;34m(self, n, df, threshold)\u001b[0m\n\u001b[1;32m     72\u001b[0m demo_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(english_text_list), size\u001b[38;5;241m=\u001b[39mn)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m demo_list:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43menglish_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menglish_text_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrench_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrench_text_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[381], line 52\u001b[0m, in \u001b[0;36mSeq2Seq_Inference.print_example\u001b[0;34m(self, i, english_text, french_text, threshold)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m french_text:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal French Text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfrench_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m emb, gen_french \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_french\u001b[49m\u001b[43m(\u001b[49m\u001b[43menglish_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m**********Generated French Text***********:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mgen_french\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# return neighbors and distances\u001b[39;00m\n","Cell \u001b[0;32mIn[381], line 28\u001b[0m, in \u001b[0;36mSeq2Seq_Inference.generate_french\u001b[0;34m(self, raw_input_text, max_len_french)\u001b[0m\n\u001b[1;32m     25\u001b[0m stop_condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop_condition:\n\u001b[0;32m---> 28\u001b[0m     preds, st \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody_encoding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     pred_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(preds[:, :, \u001b[38;5;241m2\u001b[39m:]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     30\u001b[0m     pred_word_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpp_french\u001b[38;5;241m.\u001b[39msequences_to_texts[pred_idx]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/data_adapter.py:1848\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 1, 256\nMake sure all arrays contain the same number of samples."],"ename":"ValueError","evalue":"Data cardinality is ambiguous:\n  x sizes: 1, 256\nMake sure all arrays contain the same number of samples.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}