{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Singhsansar/Machine-Translator/blob/main/Machine_Translator_French_to_English.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbcY3_Zf42dI"
      },
      "source": [
        "#**Machine Translation form English to French**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQwScRX4J47t"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSD_vLn2lsye"
      },
      "outputs": [],
      "source": [
        "#Download the trining set, dataset is from the french to english\n",
        "!wget https://raw.githubusercontent.com/futuremojo/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_train.txt\n",
        "# Download validation set pairs.\n",
        "!wget https://raw.githubusercontent.com/futuremojo/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_val.txt\n",
        "# Retrieve the test dataset.\n",
        "!wget https://raw.githubusercontent.com/futuremojo/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOtvobTB2MRr"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "from google.colab import files\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l56yetEo3DBB"
      },
      "source": [
        "##**Recurrence-based Seq2Seq Neural machine translation without Attention**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIMJOv1n25Rz"
      },
      "outputs": [],
      "source": [
        "with open('hun_eng_pairs_train.txt') as file:\n",
        "  train = [line.rstrip() for line in file]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6309rgCg3w_b"
      },
      "outputs": [],
      "source": [
        "train[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqIWrVEW3yXO"
      },
      "outputs": [],
      "source": [
        "# toatal 88647 sentences pair are there to train our model\n",
        "len(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sxul1zUR4LVx"
      },
      "outputs": [],
      "source": [
        "separator = '<sep>'\n",
        "train_input,train_input_target = map(list,zip(*[pair.split(separator) for pair in train]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlNnCTHh4xSx"
      },
      "outputs": [],
      "source": [
        "train_input[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXNLGKSk4y5O"
      },
      "outputs": [],
      "source": [
        "train_input_target[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmLQKeCK5YCq"
      },
      "outputs": [],
      "source": [
        "def normalize_unicode(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD',s)\n",
        "    if unicodedata.category(c)!='Mn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYIvhMkz6BBG"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(s):\n",
        "  s = normalize_unicode(s)\n",
        "  s = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", s)\n",
        "  s = re.sub(r'[\" \"]+', \" \", s)\n",
        "  s = s.strip()\n",
        "  return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2QNCZSi6dks"
      },
      "outputs": [],
      "source": [
        "train_data = [preprocess_sentence(s) for s in train_input]\n",
        "train_data_target = [preprocess_sentence(s) for s in train_input_target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqAfnUG_6nx_"
      },
      "outputs": [],
      "source": [
        "train_data_target[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X4dBMci7BwY"
      },
      "outputs": [],
      "source": [
        "train_data[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PMlO6Kd7Epi"
      },
      "outputs": [],
      "source": [
        "def tag_target_sentence(sentences):\n",
        "  tagged_sentence = map(lambda s: (' ').join(['<start>',s,'<end>']),sentences)\n",
        "  return list(tagged_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSVFf6FZ7kTN"
      },
      "outputs": [],
      "source": [
        "train_data_target_tagged = tag_target_sentence(train_data_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi1-L7Sa71Hf"
      },
      "outputs": [],
      "source": [
        "train_data_target_tagged[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtG0Tkme8ClK"
      },
      "outputs": [],
      "source": [
        "#tokenized the source sentence\n",
        "source_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>', filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
        "source_tokenizer.fit_on_texts(train_data)\n",
        "source_tokenizer.get_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPAoxdAF8W8O"
      },
      "outputs": [],
      "source": [
        "source_vocab_size = len(source_tokenizer.word_index)+1\n",
        "source_vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okr37i4s8rSU"
      },
      "outputs": [],
      "source": [
        "#tokenize the target sentence\n",
        "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>', filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
        "target_tokenizer.fit_on_texts(train_data_target)\n",
        "target_tokenizer.get_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJj6aPRN88En"
      },
      "outputs": [],
      "source": [
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "print(target_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBAheAcH9Psk"
      },
      "source": [
        "##**Train the encoder Input**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LjwO_nbq9L7Y"
      },
      "outputs": [],
      "source": [
        "train_encoder_inputs = source_tokenizer.texts_to_sequences(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6Iped35t9qDR"
      },
      "outputs": [],
      "source": [
        "print(train_encoder_inputs[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "amQObniu-Gpl"
      },
      "outputs": [],
      "source": [
        "print(source_tokenizer.sequences_to_texts(train_encoder_inputs[:3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Umvu9ZRb-MqM"
      },
      "outputs": [],
      "source": [
        "def generate_decoder_inputs_targets(sentences,tokenizer):\n",
        "  seqs = tokenizer.texts_to_sequences(sentences)\n",
        "  decoder_input = [s[:-1] for s in seqs]\n",
        "  decoder_target = [s[1:] for s in seqs]\n",
        "  return decoder_input , decoder_target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpPzdJaa_J0Q"
      },
      "outputs": [],
      "source": [
        "train_decoder_inputs, train_decoder_targets = generate_decoder_inputs_targets(train_data_target_tagged,target_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZNSCfx2AA71"
      },
      "outputs": [],
      "source": [
        "print(train_decoder_inputs[0], train_decoder_targets[0])\n",
        "print(target_tokenizer.sequences_to_texts(train_decoder_inputs[:1]),\n",
        "      target_tokenizer.sequences_to_texts(train_decoder_targets[:1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aetBBALlAEsV"
      },
      "outputs": [],
      "source": [
        "max_encoding_len = len(max(train_encoder_inputs, key=len))\n",
        "max_encoding_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XnUv0J_ARxv"
      },
      "outputs": [],
      "source": [
        "max_decoding_len = len(max(train_decoder_inputs, key=len))\n",
        "max_decoding_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubE5CsxxATxz"
      },
      "outputs": [],
      "source": [
        "padded_train_encoder_inputs = pad_sequences(train_encoder_inputs, max_encoding_len, padding='post', truncating='post')\n",
        "padded_train_decoder_inputs = pad_sequences(train_decoder_inputs, max_decoding_len, padding='post', truncating='post')\n",
        "padded_train_decoder_targets = pad_sequences(train_decoder_targets, max_decoding_len, padding='post', truncating='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoCcTO_GAVhi"
      },
      "outputs": [],
      "source": [
        "print(padded_train_encoder_inputs[5])\n",
        "print(padded_train_decoder_inputs[5])\n",
        "print(padded_train_decoder_targets[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq8_BLBtAXwD"
      },
      "outputs": [],
      "source": [
        "#for all the unknown words it will add unk\n",
        "target_tokenizer.sequences_to_texts([padded_train_decoder_inputs[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crYc04fVA9-Q"
      },
      "source": [
        "#**All preprocessign together for validation set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqDKmrcYBMT9"
      },
      "outputs": [],
      "source": [
        "with open('hun_eng_pairs_val.txt') as file:\n",
        "  val = [line.rstrip() for line in file]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soIMr8bNBDFu"
      },
      "outputs": [],
      "source": [
        "def process_dataset(dataset):\n",
        "\n",
        "  # Split the Hungarian and English sentences into separate lists.\n",
        "  input, output = map(list, zip(*[pair.split(separator) for pair in dataset]))\n",
        "\n",
        "  # Unicode normalization and inserting spaces around punctuation.\n",
        "  preprocessed_input = [preprocess_sentence(s) for s in input]\n",
        "  preprocessed_output = [preprocess_sentence(s) for s in output]\n",
        "\n",
        "  # Tag target sentences with <sos> and <eos> tokens.\n",
        "  tagged_preprocessed_output = tag_target_sentence(preprocessed_output)\n",
        "\n",
        "  # Vectorize encoder source sentences.\n",
        "  encoder_inputs = source_tokenizer.texts_to_sequences(preprocessed_input)\n",
        "\n",
        "  # Vectorize and create decoder input and target sentences.\n",
        "  decoder_inputs, decoder_targets = generate_decoder_inputs_targets(tagged_preprocessed_output,\n",
        "                                                                    target_tokenizer)\n",
        "\n",
        "  # Pad all collections.\n",
        "  padded_encoder_inputs = pad_sequences(encoder_inputs, max_encoding_len, padding='post', truncating='post')\n",
        "  padded_decoder_inputs = pad_sequences(decoder_inputs, max_decoding_len, padding='post', truncating='post')\n",
        "  padded_decoder_targets = pad_sequences(decoder_targets, max_decoding_len, padding='post', truncating='post')\n",
        "\n",
        "  return padded_encoder_inputs, padded_decoder_inputs, padded_decoder_targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHyzhKeTBNt8"
      },
      "outputs": [],
      "source": [
        "# Process validation dataset\n",
        "padded_val_encoder_inputs, padded_val_decoder_inputs, padded_val_decoder_targets = process_dataset(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n16en-ChBXKn"
      },
      "outputs": [],
      "source": [
        "padded_val_encoder_inputs[:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez7HLvX9B2C2"
      },
      "source": [
        "##**Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO_Wx3UlB6UC"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "default_dropout=0.2\n",
        "batch_size = 32\n",
        "epochs = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI-YeIzAF2yO"
      },
      "source": [
        "###***Encoder***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAaOphh-B7U3"
      },
      "outputs": [],
      "source": [
        "#designing the encoder for our model\n",
        "\n",
        "encoder_inputs = layers.Input(shape=[None], name ='encoder_inputs')\n",
        "encoder_embeddings = layers.Embedding(source_vocab_size,embedding_dim,mask_zero=True, name = 'encoder_embedding')\n",
        "\n",
        "#sending the encoder inputs to the encoder embedding\n",
        "encoder_embedding_output = encoder_embeddings(encoder_inputs)\n",
        "\n",
        "encoder_lstm = layers.LSTM(hidden_dim,return_state= True, dropout= default_dropout, name ='encoder_lstm')\n",
        "\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding_output)\n",
        "encoder_states = (state_h, state_c)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVAQr9T5F7ZY"
      },
      "source": [
        "###***Decoder***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfGkC8VZCGX2"
      },
      "outputs": [],
      "source": [
        "#designing the decoder for our model\n",
        "decoder_inputs = layers.Input(shape=[None], name='decoder_inputs')\n",
        "\n",
        "decoder_embeddings = layers.Embedding(target_vocab_size, embedding_dim, mask_zero= True , name='decoder_embeddin')\n",
        "\n",
        "#passing the decoder Input to the decoder_enbedding\n",
        "decoder_embedding_output = decoder_embeddings(decoder_inputs)\n",
        "\n",
        "decoder_lstm = layers.LSTM(hidden_dim, return_sequences=True,\n",
        "                           return_state =True,\n",
        "                           dropout = default_dropout,\n",
        "                           name='decoder_lstm')\n",
        "decoder_outputs,_,_ = decoder_lstm(decoder_embedding_output,initial_state=encoder_states)\n",
        "\n",
        "decoder_dense = layers.Dense(target_vocab_size, activation='softmax',name= 'decoder_dense')\n",
        "\n",
        "#The probablity distribution of the output word\n",
        "y_proba = decoder_dense(decoder_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0fH1LzwF-RR"
      },
      "source": [
        "###***Model***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1lE2faFEB6q"
      },
      "outputs": [],
      "source": [
        "#defining the model taking the encoder and the decoder together\n",
        "model = tf.keras.Model([encoder_inputs,decoder_inputs],y_proba,name='French_to_English_Without_attention')\n",
        "model.compile(optimizer='adam',loss = 'sparse_categorical_crossentropy',metrics='sparse_categorical_accuracy')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LY8TVXUIF0tr"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='hun_eng_seq2seq_nmt_no_attention.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ_0r_78HQq7"
      },
      "outputs": [],
      "source": [
        "print('encoder_inputs layer\\n input dimension {}\\n output dimension: {}'.format((batch_size, max_encoding_len), (batch_size, max_encoding_len)))\n",
        "print()\n",
        "print('encoder_embeddings layer\\n input dimension {}\\n output dimension: {}'.format((batch_size, max_encoding_len), (batch_size, max_encoding_len, embedding_dim)))\n",
        "print()\n",
        "print('encoder_lstm layer\\n input dimension {}\\n output dimension: {}'.format((batch_size, max_encoding_len, embedding_dim), [(batch_size, hidden_dim), (batch_size, hidden_dim), (batch_size, hidden_dim)]))\n",
        "print()\n",
        "print()\n",
        "print('decoder_inputs layer\\n input dimension {}\\n output dimension: {}'.format((batch_size, max_decoding_len), (batch_size, max_decoding_len)))\n",
        "print()\n",
        "print('decoder_embeddings layer\\n input dimension {}\\n output dimension: {}'.format((batch_size, max_decoding_len), (batch_size, max_decoding_len, embedding_dim)))\n",
        "print()\n",
        "print('decoder_lstm layer\\n input dimension {}\\n output dimension: {}'.format([(batch_size, max_decoding_len, embedding_dim), (batch_size, hidden_dim), (batch_size, hidden_dim)], [(batch_size, max_decoding_len, hidden_dim), (batch_size, hidden_dim), (batch_size, hidden_dim)]))\n",
        "print()\n",
        "print('decoder_dense layer(softmax)\\n input dimension {}\\n output dimension: {}'.format((batch_size, max_decoding_len, hidden_dim), (batch_size, max_decoding_len, target_vocab_size)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSrIDUvvHjX4"
      },
      "outputs": [],
      "source": [
        "# Saving this to a folder on my local machine.\n",
        "filepath=\"/content/drive/MyDrive/NLP/HunEngNMTNoAttention/training1/cp.ckpt\"\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=filepath,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7VO5AhgHrqn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "986acee6-c21d-4e8c-8dcd-74693bc949e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2771/2771 [==============================] - ETA: 0s - loss: 4.7886 - sparse_categorical_accuracy: 0.3845\n",
            "Epoch 1: saving model to /content/drive/MyDrive/NLP/HunEngNMTNoAttention/training1/cp.ckpt\n",
            "2771/2771 [==============================] - 276s 93ms/step - loss: 4.7886 - sparse_categorical_accuracy: 0.3845 - val_loss: 6.5986 - val_sparse_categorical_accuracy: 0.2263\n",
            "Epoch 2/20\n",
            "2771/2771 [==============================] - ETA: 0s - loss: 3.4738 - sparse_categorical_accuracy: 0.4888\n",
            "Epoch 2: saving model to /content/drive/MyDrive/NLP/HunEngNMTNoAttention/training1/cp.ckpt\n",
            "2771/2771 [==============================] - 245s 89ms/step - loss: 3.4738 - sparse_categorical_accuracy: 0.4888 - val_loss: 7.1184 - val_sparse_categorical_accuracy: 0.2419\n",
            "Epoch 3/20\n",
            "1208/2771 [============>.................] - ETA: 2:05 - loss: 2.7852 - sparse_categorical_accuracy: 0.5353"
          ]
        }
      ],
      "source": [
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "history = model.fit([padded_train_encoder_inputs, padded_train_decoder_inputs], padded_train_decoder_targets,\n",
        "                     batch_size=batch_size,\n",
        "                     epochs=20, #changed\n",
        "                     validation_data=([padded_val_encoder_inputs, padded_val_decoder_inputs], padded_val_decoder_targets),\n",
        "                     callbacks=[cp_callback, es_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPw3v_9nIGx3"
      },
      "outputs": [],
      "source": [
        "model.save(\"/content/drive/MyDrive/NLP/French_to_English_Without_attention\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ndMzgGYfKIZ"
      },
      "outputs": [],
      "source": [
        "#### Zip and download the model.\n",
        "!zip -r /content/drive/MyDrive/NLP/English_to_French_Without_attention.zip /content/drive/MyDrive/NLP/English_to_French_Without_attention\n",
        "files.download(\"/content/drive/MyDrive/NLP/French_to_English_Without_attention\")\n",
        "\n",
        "\n",
        "##### Save the tokenizers as JSON files. can be used now\n",
        "source_tokenizer_json = source_tokenizer.to_json()\n",
        "with io.open('source_tokenizer.json', 'w', encoding='utf-8') as f:\n",
        "  f.write(json.dumps(source_tokenizer_json, ensure_ascii=False))\n",
        "\n",
        "target_tokenizer_json = target_tokenizer.to_json()\n",
        "with io.open('target_tokenizer.json', 'w', encoding='utf-8') as f:\n",
        "  f.write(json.dumps(target_tokenizer_json, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2eEcp9ugxx0"
      },
      "outputs": [],
      "source": [
        "##### Save the tokenizers as JSON files. The resulting files can be downloaded by left-clicking on them. for the further use only\n",
        "source_tokenizer_json = source_tokenizer.to_json()\n",
        "with io.open('/content/drive/MyDrive/NLP/source_tokenizer.json', 'w', encoding='utf-8') as f:\n",
        "  f.write(json.dumps(source_tokenizer_json, ensure_ascii=False))\n",
        "\n",
        "target_tokenizer_json = target_tokenizer.to_json()\n",
        "with io.open('/content/drive/MyDrive/NLP/target_tokenizer.json', 'w', encoding='utf-8') as f:\n",
        "  f.write(json.dumps(target_tokenizer_json, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgBLIvhBK64i"
      },
      "outputs": [],
      "source": [
        "def translate_without_attention(sentence: str,\n",
        "                                source_tokenizer, encoder,\n",
        "                                target_tokenizer, decoder,\n",
        "                                max_translated_len = 30):\n",
        "\n",
        "  # Vectorize the source sentence and run it through the encoder.\n",
        "  input_seq = source_tokenizer.texts_to_sequences([sentence])\n",
        "\n",
        "  # Get the tokenized sentence to see if there are any unknown tokens.\n",
        "  tokenized_sentence = source_tokenizer.sequences_to_texts(input_seq)\n",
        "\n",
        "  states = encoder.predict(input_seq)\n",
        "\n",
        "  current_word = '<sos>'\n",
        "  decoded_sentence = []\n",
        "\n",
        "  while len(decoded_sentence) < max_translated_len:\n",
        "\n",
        "    # Set the next input word for the decoder.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = target_tokenizer.word_index[current_word]\n",
        "\n",
        "    # Determine the next word.\n",
        "    target_y_proba, h, c = decoder.predict([target_seq] + states)\n",
        "    target_token_index = np.argmax(target_y_proba[0, -1, :])\n",
        "    current_word = target_tokenizer.index_word[target_token_index]\n",
        "\n",
        "    if (current_word == '<eos>'):\n",
        "      break\n",
        "\n",
        "    decoded_sentence.append(current_word)\n",
        "    states = [h, c]\n",
        "\n",
        "  return tokenized_sentence[0], ' '.join(decoded_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXBVY89BhbJS"
      },
      "source": [
        "#**Testing the Modal Predection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns5rYM5jhnix"
      },
      "outputs": [],
      "source": [
        "with open('source_tokenizer.json') as f:\n",
        "    data = json.load(f)\n",
        "    source_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
        "\n",
        "with open('target_tokenizer.json') as f:\n",
        "    data = json.load(f)\n",
        "    target_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "we-KraYMhsZR"
      },
      "outputs": [],
      "source": [
        "# Load the model.\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/NLP/French_to_English_Without_attention')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_03xBiTh76L"
      },
      "source": [
        "###The *test* dataset contains sentences (and most certainly words) unseen by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFcTF5QoiC4l"
      },
      "outputs": [],
      "source": [
        "with open('hun_eng_pairs_test.txt') as file:\n",
        "  test = [line.rstrip() for line in file]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbEQeq9JiGFD"
      },
      "outputs": [],
      "source": [
        "test[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KspZe4f-iJ5K"
      },
      "outputs": [],
      "source": [
        "# Preprocess test dataset\n",
        "padded_test_encoder_inputs, padded_test_decoder_inputs, padded_test_decoder_targets = process_dataset(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2tY9cgEiX5j"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set.\n",
        "model.evaluate([padded_test_encoder_inputs, padded_test_decoder_inputs], padded_test_decoder_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYaTgtlDibP2"
      },
      "outputs": [],
      "source": [
        "# These are the layers of our trained model.\n",
        "[layer.name for layer in model.layers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hssDbGtQikL6"
      },
      "outputs": [],
      "source": [
        "encoder_inputs = model.get_layer('encoder_inputs').input\n",
        "\n",
        "encoder_embedding_layer = model.get_layer('encoder_embedding')\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_inputs)\n",
        "\n",
        "encoder_lstm = model.get_layer('encoder_lstm')\n",
        "\n",
        "_, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embeddings)\n",
        "\n",
        "encoder_states = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "# Our stand-alone encoder model. encoder_inputs is the input to the encoder,\n",
        "# and encoder_states is the expected output.\n",
        "encoder_model_no_attention = tf.keras.Model(encoder_inputs, encoder_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQnoFaSQi2uX"
      },
      "outputs": [],
      "source": [
        "plot_model(encoder_model_no_attention, to_file='encoder_model_no_attention_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1P3_YPljJEt"
      },
      "outputs": [],
      "source": [
        "decoder_inputs = model.get_layer('decoder_inputs').input\n",
        "\n",
        "decoder_embedding_layer = model.get_layer('decoder_embeddin')\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "# Inputs to represent the decoder's LSTM hidden and cell states. We'll populate\n",
        "# these manually using the encoder's output for the initial state.\n",
        "decoder_input_state_h = tf.keras.Input(shape=(hidden_dim,), name='decoder_input_state_h')\n",
        "decoder_input_state_c = tf.keras.Input(shape=(hidden_dim,), name='decoder_input_state_c')\n",
        "decoder_input_states = [decoder_input_state_h, decoder_input_state_c]\n",
        "\n",
        "decoder_lstm = model.get_layer('decoder_lstm')\n",
        "\n",
        "decoder_sequence_outputs, decoder_output_state_h, decoder_output_state_c = decoder_lstm(\n",
        "    decoder_embeddings, initial_state=decoder_input_states\n",
        ")\n",
        "\n",
        "# Update hidden and cell states for the next time step.\n",
        "decoder_output_states = [decoder_output_state_h, decoder_output_state_c]\n",
        "\n",
        "decoder_dense = model.get_layer('decoder_dense')\n",
        "y_proba = decoder_dense(decoder_sequence_outputs)\n",
        "\n",
        "decoder_model_no_attention = tf.keras.Model(\n",
        "    [decoder_inputs] + decoder_input_states,\n",
        "    [y_proba] + decoder_output_states\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxUb7QerjM98"
      },
      "outputs": [],
      "source": [
        "def translate_without_attention(sentence: str,\n",
        "                                source_tokenizer, encoder,\n",
        "                                target_tokenizer, decoder,\n",
        "                                max_translated_len = 30):\n",
        "\n",
        "  # Vectorize the source sentence and run it through the encoder.\n",
        "  input_seq = source_tokenizer.texts_to_sequences([sentence])\n",
        "\n",
        "  # Get the tokenized sentence to see if there are any unknown tokens.\n",
        "  tokenized_sentence = source_tokenizer.sequences_to_texts(input_seq)\n",
        "\n",
        "  states = encoder.predict(input_seq)\n",
        "\n",
        "  current_word = '<start>'\n",
        "  decoded_sentence = []\n",
        "\n",
        "\n",
        "  #matching the predection in to the bag of words\n",
        "  while len(decoded_sentence) < max_translated_len:\n",
        "\n",
        "    # Set the next input word for the decoder.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = target_tokenizer.word_index[current_word]\n",
        "\n",
        "    # Determine the next word.\n",
        "    target_y_proba, h, c = decoder.predict([target_seq] + states)\n",
        "    target_token_index = np.argmax(target_y_proba[0, -1, :])\n",
        "    current_word = target_tokenizer.index_word[target_token_index]\n",
        "\n",
        "    if (current_word == '<end>'):\n",
        "      break\n",
        "\n",
        "    decoded_sentence.append(current_word)\n",
        "    states = [h, c]\n",
        "\n",
        "  return tokenized_sentence[0], ' '.join(decoded_sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "641zxthLjbDA"
      },
      "outputs": [],
      "source": [
        "# random.seed is just here to re-create results.\n",
        "random.seed(1)\n",
        "sentences = random.sample(test, 15)\n",
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf6_9Y9ejeru"
      },
      "outputs": [],
      "source": [
        "def translate_sentences(sentences, translation_func, source_tokenizer, encoder,\n",
        "                        target_tokenizer, decoder):\n",
        "  translations = {'Tokenized Original': [], 'Reference': [], 'Translation': []}\n",
        "\n",
        "  for s in sentences:\n",
        "    source, target = s.split(separator)\n",
        "    source = preprocess_sentence(source)\n",
        "    tokenized_sentence, translated = translation_func(source, source_tokenizer, encoder,\n",
        "                                                      target_tokenizer, decoder)\n",
        "\n",
        "    translations['Tokenized Original'].append(tokenized_sentence)\n",
        "    translations['Reference'].append(target)\n",
        "    translations['Translation'].append(translated)\n",
        "\n",
        "  return translations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDNrSTDWjgpV"
      },
      "outputs": [],
      "source": [
        "translations_no_attention = pd.DataFrame(translate_sentences(sentences, translate_without_attention,\n",
        "                                                             source_tokenizer, encoder_model_no_attention,\n",
        "                                                             target_tokenizer, decoder_model_no_attention))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRPPjHYSnN1D"
      },
      "outputs": [],
      "source": [
        "translations_no_attention"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJZg6GeT73sLYD0uM/BiNw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}